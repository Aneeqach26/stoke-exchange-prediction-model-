# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NqTmTzKi3JKKxzStp6J8YEuznfal1ZHn
"""

import requests
import yfinance as yf
from textblob import TextBlob
import pandas as pd
import numpy as np
import datetime
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ðŸ”‘ Replace with your own News API key
NEWS_API_KEY = "e6180eed2566408c87359cf9e8c8662a"

def get_sentiment_score(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

def fetch_daily_sentiment(date, query="stock"):
    url = (
        f"https://newsapi.org/v2/everything?"
        f"q={query}&from={date}&to={date}&sortBy=popularity&apiKey={NEWS_API_KEY}"
    )
    response = requests.get(url)
    sentiment = 0
    count = 0
    if response.status_code == 200:
        articles = response.json().get("articles", [])
        for article in articles:
            headline = article.get("title", "")
            if headline:
                sentiment += get_sentiment_score(headline)
                count += 1
    return sentiment / count if count > 0 else 0

tickers = {
    "^NSEI": "Nifty",
    "^GSPC": "S&P 500",
    "^FTSE": "FTSE 100"
}

start_date = "2022-01-01"
end_date = datetime.datetime.today().strftime('%Y-%m-%d')

for ticker, name in tickers.items():
    print(f"\nðŸš€ Processing {name} ({ticker})")

    # Fetch stock data
    data = yf.download(ticker, start=start_date, end=end_date)

    # Technical indicators
    data['SMA20'] = data['Close'].rolling(window=20).mean()
    data['SMA50'] = data['Close'].rolling(window=50).mean()
    data['EMA20'] = data['Close'].ewm(span=20, adjust=False).mean()
    data['EMA50'] = data['Close'].ewm(span=50, adjust=False).mean()

    delta = data['Close'].diff()
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.rolling(window=14).mean()
    avg_loss = loss.rolling(window=14).mean()
    rs = avg_gain / avg_loss
    data['RSI'] = 100 - (100 / (1 + rs))

    data['EMA12'] = data['Close'].ewm(span=12, adjust=False).mean()
    data['EMA26'] = data['Close'].ewm(span=26, adjust=False).mean()
    data['MACD'] = data['EMA12'] - data['EMA26']
    data['MACD_signal'] = data['MACD'].ewm(span=9, adjust=False).mean()

    data['Middle_Band'] = data['SMA20']
    data['StdDev'] = data['Close'].rolling(window=20).std()
    data['Upper_Band'] = data['Middle_Band'] + (2 * data['StdDev'])
    data['Lower_Band'] = data['Middle_Band'] - (2 * data['StdDev'])

    # Target variable
    data['Target'] = data['Close']

    # Sentiment analysis
    sentiment_scores = []
    for date in data.index:
        sentiment = fetch_daily_sentiment(date.strftime('%Y-%m-%d'), query=name)
        sentiment_scores.append(sentiment)

    data['Sentiment'] = sentiment_scores
    data['Sentiment'] = data['Sentiment'].shift(1)
    data.dropna(inplace=True)

    # Feature and target
    features = ['SMA20', 'SMA50', 'EMA20', 'EMA50', 'RSI', 'MACD', 'Upper_Band', 'Lower_Band', 'Sentiment']
    X = data[features]
    y = data['Target']

    # Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Evaluation function
    def evaluate_model(name, y_true, y_pred):
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

        print(f"\nðŸ“Š {name} Evaluation:")
        print(f"MAE: {mae:.4f}")
        print(f"MSE: {mse:.4f}")
        print(f"RMSE: {rmse:.4f}")
        print(f"RÂ² Score: {r2:.4f}")
        print(f"MAPE: {mape:.2f}%")

    # Models
    rf = RandomForestRegressor(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)
    evaluate_model(f"{name} - Random Forest", y_test, y_pred_rf)

    gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
    gb.fit(X_train, y_train)
    y_pred_gb = gb.predict(X_test)
    evaluate_model(f"{name} - Gradient Boosting", y_test, y_pred_gb)

    # Plot predictions
    plt.figure(figsize=(12,6))
    plt.plot(y_test.index, y_test, label='Actual', color='black')
    plt.plot(y_test.index, y_pred_rf, label='RF Prediction', linestyle='--', color='blue')
    plt.plot(y_test.index, y_pred_gb, label='GB Prediction', linestyle='--', color='orange')
    plt.title(f'{name}: Actual vs Predicted Closing Prices')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

import yfinance as yf
import pandas as pd
import numpy as np

# Stock select karna (example ke liye NIFTY 50)
ticker = "^NSEI"
start_date = "2022-01-01"
end_date = "2024-12-31"

# Stock data download
data = yf.download(ticker, start=start_date, end=end_date)

# Technical indicators banana (jaise pehle aap kar rahe the)
data['SMA20'] = data['Close'].rolling(window=20).mean()
data['SMA50'] = data['Close'].rolling(window=50).mean()
data['EMA20'] = data['Close'].ewm(span=20, adjust=False).mean()
data['EMA50'] = data['Close'].ewm(span=50, adjust=False).mean()

delta = data['Close'].diff()
gain = delta.where(delta > 0, 0)
loss = -delta.where(delta < 0, 0)
avg_gain = gain.rolling(window=14).mean()
avg_loss = loss.rolling(window=14).mean()
rs = avg_gain / avg_loss
data['RSI'] = 100 - (100 / (1 + rs))

data['EMA12'] = data['Close'].ewm(span=12, adjust=False).mean()
data['EMA26'] = data['Close'].ewm(span=26, adjust=False).mean()
data['MACD'] = data['EMA12'] - data['EMA26']
data['MACD_signal'] = data['MACD'].ewm(span=9, adjust=False).mean()

data['Middle_Band'] = data['SMA20']
data['StdDev'] = data['Close'].rolling(window=20).std()
data['Upper_Band'] = data['Middle_Band'] + (2 * data['StdDev'])
data['Lower_Band'] = data['Middle_Band'] - (2 * data['StdDev'])

# Target price
data['Target'] = data['Close']

# NaN values hata do
data.dropna(inplace=True)

# Feature columns define karo
features = ['SMA20', 'SMA50', 'EMA20', 'EMA50', 'RSI', 'MACD', 'Upper_Band', 'Lower_Band']

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split

X = data[features]
y = data['Target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Model
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Gradient Boosting Model
gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
gb.fit(X_train, y_train)

# Prepare last available feature row for next day prediction
latest_features = data[features].iloc[-1]
latest_features = np.array(latest_features).reshape(1, -1)

# Random Forest prediction
next_day_pred_rf = rf.predict(latest_features)
print(f" Random Forest Next Day Predicted Closing Price: ${next_day_pred_rf[0]:.2f}")

# Gradient Boosting prediction
next_day_pred_gb = gb.predict(latest_features)
print(f" Gradient Boosting Next Day Predicted Closing Price: ${next_day_pred_gb[0]:.2f}")

!pip install streamlit
!pip install pyngrok
!pip install joblib

import joblib
from sklearn.ensemble import RandomForestRegressor

# Suppose your X_train and y_train are already ready
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Save model
joblib.dump(rf, 'rf_model.pkl')

# Save feature list
features_list = ['SMA20', 'SMA50', 'EMA20', 'EMA50', 'RSI', 'MACD', 'Upper_Band', 'Lower_Band', 'Sentiment']
joblib.dump(features_list, 'features.pkl')

print(" Model and features saved successfully!")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import joblib
# import numpy as np
# 
# # Load model and features
# model = joblib.load('rf_model.pkl')
# features = joblib.load('features.pkl')
# 
# st.title("ðŸ“ˆ Stock Price Prediction App")
# 
# # User inputs
# st.header("ðŸ”® Enter Feature Values")
# user_input = []
# for feature in features:
#     value = st.number_input(f"Enter {feature}:", value=0.0)
#     user_input.append(value)
# 
# # Prediction
# if st.button('Predict'):
#     input_array = np.array(user_input).reshape(1, -1)
#     prediction = model.predict(input_array)
#     st.success(f"Predicted Closing Price: ${prediction[0]:.2f}")
#

!pip install streamlit
!pip install pyngrok
!pip install joblib

from pyngrok import ngrok

# Streamlit app ko background mein chalao
!streamlit run app.py &

# Public URL generate karo
public_url = ngrok.connect(port=8501)
print(f" Public URL: {public_url}")
